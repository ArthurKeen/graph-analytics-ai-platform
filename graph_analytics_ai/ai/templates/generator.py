"""
GAE template generator.

Converts use cases into executable GAE analysis templates.
"""

from typing import List, Optional, Dict, Any

from ..generation.use_cases import UseCase, UseCaseType
from ..schema.models import GraphSchema, SchemaAnalysis
from .models import (
    AnalysisTemplate,
    AlgorithmType,
    AlgorithmParameters,
    TemplateConfig,
    EngineSize,
    DEFAULT_ALGORITHM_PARAMS,
    recommend_engine_size
)


# Mapping from use case types to algorithm types
# Only includes algorithms that are actually supported by GAE
USE_CASE_TO_ALGORITHM = {
    UseCaseType.CENTRALITY: [
        AlgorithmType.PAGERANK
    ],
    UseCaseType.COMMUNITY: [
        AlgorithmType.WCC,
        AlgorithmType.SCC,
        AlgorithmType.LABEL_PROPAGATION
    ],
    UseCaseType.PATHFINDING: [
        AlgorithmType.PAGERANK  # Best available for path/influence analysis
    ],
    UseCaseType.PATTERN: [
        AlgorithmType.WCC,  # For connected pattern detection
        AlgorithmType.LABEL_PROPAGATION
    ],
    UseCaseType.ANOMALY: [
        AlgorithmType.WCC,  # For anomaly clusters
        AlgorithmType.PAGERANK  # For anomalous influence patterns
    ],
    UseCaseType.RECOMMENDATION: [
        AlgorithmType.PAGERANK  # For recommendation scoring
    ],
    UseCaseType.SIMILARITY: [
        AlgorithmType.WCC,
        AlgorithmType.LABEL_PROPAGATION
    ]
}


class TemplateGenerator:
    """
    Generates GAE analysis templates from use cases.
    
    Converts high-level use case descriptions into executable
    GAE analysis configurations with optimized parameters.
    
    Example:
        >>> from graph_analytics_ai.ai.templates import TemplateGenerator
        >>> from graph_analytics_ai.ai.generation import generate_use_cases
        >>> 
        >>> generator = TemplateGenerator(graph_name="my_graph")
        >>> use_cases = generate_use_cases(requirements, schema_analysis)
        >>> templates = generator.generate_templates(use_cases, schema)
        >>> 
        >>> for template in templates:
        ...     print(f"{template.name}: {template.algorithm.algorithm.value}")
    """
    
    def __init__(
        self,
        graph_name: str = "ecommerce_graph",
        default_engine_size: EngineSize = EngineSize.SMALL,
        auto_optimize: bool = True
    ):
        """
        Initialize template generator.
        
        Args:
            graph_name: Name of the graph to analyze
            default_engine_size: Default engine size if not optimized
            auto_optimize: Whether to auto-optimize parameters
        """
        self.graph_name = graph_name
        self.default_engine_size = default_engine_size
        self.auto_optimize = auto_optimize
    
    def generate_templates(
        self,
        use_cases: List[UseCase],
        schema: Optional[GraphSchema] = None,
        schema_analysis: Optional[SchemaAnalysis] = None
    ) -> List[AnalysisTemplate]:
        """
        Generate analysis templates from use cases.
        
        Args:
            use_cases: List of use cases to convert
            schema: Optional graph schema for optimization
            schema_analysis: Optional schema analysis for insights
            
        Returns:
            List of analysis templates ready for execution
        """
        templates = []
        
        for use_case in use_cases:
            # Get suitable algorithms for this use case type
            algorithms = USE_CASE_TO_ALGORITHM.get(
                use_case.use_case_type,
                [AlgorithmType.PAGERANK]  # Default fallback
            )
            
            # Generate template for primary algorithm
            primary_algo = algorithms[0]
            template = self._create_template(
                use_case=use_case,
                algorithm_type=primary_algo,
                schema=schema,
                schema_analysis=schema_analysis
            )
            templates.append(template)
        
        return templates
    
    def _create_template(
        self,
        use_case: UseCase,
        algorithm_type: AlgorithmType,
        schema: Optional[GraphSchema] = None,
        schema_analysis: Optional[SchemaAnalysis] = None
    ) -> AnalysisTemplate:
        """Create a single analysis template."""
        
        # Get base parameters for algorithm
        params = DEFAULT_ALGORITHM_PARAMS.get(algorithm_type, {}).copy()
        
        # Optimize parameters if requested
        if self.auto_optimize and schema:
            params = self._optimize_parameters(
                algorithm_type=algorithm_type,
                params=params,
                schema=schema,
                schema_analysis=schema_analysis
            )
        
        # Create algorithm parameters
        algorithm = AlgorithmParameters(
            algorithm=algorithm_type,
            parameters=params
        )
        
        # Determine engine size
        engine_size = self._determine_engine_size(schema)
        
        # Extract collections from use case data needs
        vertex_collections, edge_collections = self._extract_collections(use_case, schema)
        
        # Use CollectionSelector to choose algorithm-appropriate collections
        selection_metadata = {}
        if schema and schema.vertex_collections:
            collection_selection = self.collection_selector.select_collections(
                algorithm=algorithm_type,
                schema=schema,
                collection_hints=self.collection_hints if self.collection_hints else None,
                use_case_context=use_case.description
            )
            
            # Override with algorithm-specific selection
            vertex_collections = collection_selection.vertex_collections
            edge_collections = collection_selection.edge_collections
            
            # Store selection reasoning in template metadata
            selection_metadata = {
                "collection_selection_reasoning": collection_selection.reasoning,
                "excluded_collections": {
                    "vertices": collection_selection.excluded_vertices,
                    "edges": collection_selection.excluded_edges
                },
                "estimated_graph_size": collection_selection.estimated_graph_size
            }
        elif (not vertex_collections or not edge_collections) and schema:
            # Fallback if selector can't be used
            if not vertex_collections and schema.vertex_collections:
                available = [
                    name for name, coll in schema.vertex_collections.items()
                    if not any(pattern in name.lower() for pattern in ['uc_', 'result', '_temp'])
                    and coll.document_count > 1000  # Only substantial collections
                ]
                vertex_collections = available[:5]  # Limit to first 5 substantial ones
            if not edge_collections and schema.edge_collections:
                edge_collections = list(schema.edge_collections.keys())[:5]
        
        # Create template config
        config = TemplateConfig(
            graph_name=self.graph_name,
            vertex_collections=vertex_collections,
            edge_collections=edge_collections,
            engine_size=engine_size,
            store_results=True,
            result_collection=f"{use_case.id.lower().replace('-', '_')}_results"
        )
        
        # Estimate runtime (basic heuristic)
        estimated_runtime = self._estimate_runtime(
            algorithm_type=algorithm_type,
            schema=schema
        )
        
        # Create template
        template = AnalysisTemplate(
            name=f"{use_case.id}: {use_case.title}",
            description=use_case.description,
            algorithm=algorithm,
            config=config,
            use_case_id=use_case.id,
            estimated_runtime_seconds=estimated_runtime,
            metadata={
                "priority": use_case.priority.value,
                "use_case_type": use_case.use_case_type.value,
                "algorithms": use_case.graph_algorithms,
                "success_metrics": use_case.success_metrics,
                **selection_metadata  # Include collection selection info
            }
        )
        
        return template
    
    def _optimize_parameters(
        self,
        algorithm_type: AlgorithmType,
        params: Dict[str, Any],
        schema: GraphSchema,
        schema_analysis: Optional[SchemaAnalysis] = None
    ) -> Dict[str, Any]:
        """
        Optimize algorithm parameters based on graph characteristics.
        
        Args:
            algorithm_type: Algorithm to optimize for
            params: Base parameters
            schema: Graph schema
            schema_analysis: Optional analysis for insights
            
        Returns:
            Optimized parameters
        """
        optimized = params.copy()
        
        # Get graph stats
        total_docs = schema.total_documents
        total_edges = schema.total_edges
        avg_degree = (2 * total_edges) / total_docs if total_docs > 0 else 0
        
        # Algorithm-specific optimizations
        if algorithm_type == AlgorithmType.PAGERANK:
            # Adjust iterations based on graph size
            if total_docs > 10000:
                optimized["maximum_supersteps"] = 50  # Fewer for large graphs
            elif total_docs < 1000:
                optimized["maximum_supersteps"] = 150  # More for small graphs
            
            # Adjust threshold based on density
            if avg_degree > 10:
                optimized["threshold"] = 0.0005  # Tighter for dense graphs
        
        elif algorithm_type == AlgorithmType.LABEL_PROPAGATION:
            # Adjust iterations based on graph size
            if total_docs > 10000:
                optimized["max_iterations"] = 30  # Fewer for large graphs
            elif total_docs < 1000:
                optimized["max_iterations"] = 100  # More for small graphs
        
        elif algorithm_type == AlgorithmType.WCC:
            # WCC doesn't have many tunable parameters
            # Result store name is set elsewhere
            pass
        
        elif algorithm_type == AlgorithmType.BETWEENNESS_CENTRALITY:
            # For very large graphs, might want to sample
            if total_docs > 50000:
                optimized["sample_size"] = 1000  # Sample nodes
        
        return optimized
    
    def _determine_engine_size(self, schema: Optional[GraphSchema]) -> EngineSize:
        """Determine appropriate engine size for the graph."""
        if not schema:
            return self.default_engine_size
        
        return recommend_engine_size(
            vertex_count=schema.total_documents,
            edge_count=schema.total_edges
        )
    
    def _extract_collections(self, use_case: UseCase, schema: Optional[GraphSchema] = None) -> tuple:
        """
        Extract vertex and edge collections from use case and schema.
        
        IMPORTANT: For proper graph analysis, we need to avoid:
        - Satellite collections that artificially connect everything
        - Result collections from previous analyses  
        - Hub collections that aren't relevant to the use case
        
        Args:
            use_case: Use case with data requirements
            schema: Optional schema for available collections
            
        Returns:
            tuple: (vertex_collections, edge_collections)
        """
        vertex_collections = []
        edge_collections = []
        
        # If schema provided, get available collections
        available_vertices = set()
        available_edges = set()
        
        if schema:
            if hasattr(schema, 'vertex_collections') and schema.vertex_collections:
                available_vertices = set(schema.vertex_collections.keys())
            if hasattr(schema, 'edge_collections') and schema.edge_collections:
                available_edges = set(schema.edge_collections.keys())
        
        # Filter out result collections and common satellite collections
        exclude_patterns = ['uc_', 'result', '_temp', '_backup']
        
        # Parse data needs from use case
        for need in use_case.data_needs:
            need_lower = need.lower()
            
            # Device and IP focused (household resolution)
            if "device" in need_lower and "Device" in available_vertices:
                vertex_collections.append("Device")
            if "ip" in need_lower and "IP" in available_vertices:
                vertex_collections.append("IP")
            
            # Content focused (inventory, apps, sites)
            if ("app" in need_lower or "product" in need_lower) and "AppProduct" in available_vertices:
                vertex_collections.append("AppProduct")
            if ("site" in need_lower or "web" in need_lower) and "Site" in available_vertices:
                vertex_collections.append("Site")
            if "installedapp" in need_lower and "InstalledApp" in available_vertices:
                vertex_collections.append("InstalledApp")
            if "siteuse" in need_lower and "SiteUse" in available_vertices:
                vertex_collections.append("SiteUse")
            
            # Publisher/content providers (but be careful - these can be hubs)
            if "publisher" in need_lower and "Publisher" in available_vertices:
                vertex_collections.append("Publisher")
            
            # Edges
            if "seen_on_ip" in need_lower or ("device" in need_lower and "ip" in need_lower):
                if "SEEN_ON_IP" in available_edges:
                    edge_collections.append("SEEN_ON_IP")
            if "seen_on_app" in need_lower or "app" in need_lower:
                if "SEEN_ON_APP" in available_edges:
                    edge_collections.append("SEEN_ON_APP")
            if "seen_on_site" in need_lower or "site" in need_lower:
                if "SEEN_ON_SITE" in available_edges:
                    edge_collections.append("SEEN_ON_SITE")
            if "instance_of" in need_lower:
                if "INSTANCE_OF" in available_edges:
                    edge_collections.append("INSTANCE_OF")
        
        # If nothing extracted from data_needs, infer from use case type and title
        if not vertex_collections and not edge_collections:
            title_lower = use_case.title.lower()
            desc_lower = use_case.description.lower()
            combined = f"{title_lower} {desc_lower}"
            
            # Household/identity resolution use cases
            if any(term in combined for term in ["household", "identity", "resolution", "device", "ip"]):
                if "Device" in available_vertices:
                    vertex_collections.append("Device")
                if "IP" in available_vertices:
                    vertex_collections.append("IP")
                if "SEEN_ON_IP" in available_edges:
                    edge_collections.append("SEEN_ON_IP")
            
            # Fraud/anomaly detection
            elif any(term in combined for term in ["fraud", "anomaly", "bot"]):
                if "Device" in available_vertices:
                    vertex_collections.append("Device")
                if "IP" in available_vertices:
                    vertex_collections.append("IP")
                if "SEEN_ON_IP" in available_edges:
                    edge_collections.append("SEEN_ON_IP")
            
            # Content/inventory analysis
            elif any(term in combined for term in ["content", "inventory", "app", "site", "publisher"]):
                if "AppProduct" in available_vertices:
                    vertex_collections.append("AppProduct")
                if "Site" in available_vertices:
                    vertex_collections.append("Site")
                if "Device" in available_vertices:
                    vertex_collections.append("Device")
                if "SEEN_ON_APP" in available_edges:
                    edge_collections.append("SEEN_ON_APP")
                if "SEEN_ON_SITE" in available_edges:
                    edge_collections.append("SEEN_ON_SITE")
        
        # Remove duplicates while preserving order
        vertex_collections = list(dict.fromkeys(vertex_collections))
        edge_collections = list(dict.fromkeys(edge_collections))
        
        # Filter out excluded patterns
        vertex_collections = [
            v for v in vertex_collections 
            if not any(pattern in v.lower() for pattern in exclude_patterns)
        ]
        
        return vertex_collections, edge_collections
    
    def _estimate_runtime(
        self,
        algorithm_type: AlgorithmType,
        schema: Optional[GraphSchema]
    ) -> Optional[float]:
        """Estimate runtime in seconds (very rough heuristic)."""
        if not schema:
            return None
        
        n = schema.total_documents
        m = schema.total_edges
        
        # Very rough complexity estimates
        if algorithm_type == AlgorithmType.PAGERANK:
            # O(iterations * m)
            return max(1.0, (n + m) / 10000)  # ~10k elements per second
        
        elif algorithm_type == AlgorithmType.LABEL_PROPAGATION:
            # O(iterations * m)
            return max(1.5, (n + m) / 8000)
        
        elif algorithm_type == AlgorithmType.WCC:
            # O(m) - usually fast
            return max(0.5, (n + m) / 15000)
        
        elif algorithm_type == AlgorithmType.SCC:
            # O(m) - similar to WCC
            return max(0.5, (n + m) / 15000)
        
        elif algorithm_type == AlgorithmType.BETWEENNESS_CENTRALITY:
            # O(n * m) - can be slow
            return max(5.0, (n * m) / 100000)
        
        else:
            # Default estimate
            return max(1.0, (n + m) / 15000)


def generate_template(
    use_case: UseCase,
    graph_name: str = "ecommerce_graph",
    schema: Optional[GraphSchema] = None,
    algorithm_type: Optional[AlgorithmType] = None
) -> AnalysisTemplate:
    """
    Convenience function to generate a single template.
    
    Args:
        use_case: Use case to convert
        graph_name: Name of the graph
        schema: Optional schema for optimization
        algorithm_type: Optional specific algorithm (auto-detected if None)
        
    Returns:
        Analysis template
    """
    generator = TemplateGenerator(graph_name=graph_name)
    
    if algorithm_type is None:
        # Auto-detect algorithm
        algorithms = USE_CASE_TO_ALGORITHM.get(
            use_case.use_case_type,
            [AlgorithmType.PAGERANK]
        )
        algorithm_type = algorithms[0]
    
    return generator._create_template(
        use_case=use_case,
        algorithm_type=algorithm_type,
        schema=schema
    )

